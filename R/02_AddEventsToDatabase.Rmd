---
title: "AddGgEventsToDatabase"
output: html_document
date: "2025-05-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PAMpal Data Processing

Start by loading the required packages

```{r, load packages, message=FALSE}
library("easypackages")
library("lubridate")
libraries("PAMpal","PAMmisc","dplyr",  "here",'beepr')
here()
```

1.  Define details of deployment to process

    ```{r, Set deployment metadata}
    ###USER-DEFINED FIELDS#### 
    baseDir <- here('data')
    DriftID<-'ADRIFT_055'
    binFolder <- paste0(baseDir,'/Binaries/',DriftID)    #Folder with binaries
    # this database should be a COPY of the original because we will add events to it later
    db <- paste0(baseDir,'/Databases/',DriftID,' - Copy.sqlite3')

    ```

2.  [Load in acoustic study from an RDS file and Gg times from log]{.underline}

    ```{r, read acoustic study, include=FALSE}
    data<-readRDS(paste0(baseDir,'/AcousticStudies/',DriftID,'_Gg.rds'))
    #Update known location of database and binaries for this acoustic study
    data<-updateFiles(data)

    # Double check warning messages
    print(getWarnings(data)$message)

    #Import known Risso's dolphin Event Times
    GgTimes<-read.csv(here('EventTimes','ADRIFT_GgDetections.csv'))
    GgTimes$start<-as.POSIXct(GgTimes$UTC,format='%Y-%m-%d %H:%M:%S',tz='UTC')
    GgTimes$end<-as.POSIXct(GgTimes$end,format='%Y-%m-%d %H:%M:%S',tz='UTC')
    DriftTimes<-filter(GgTimes,DriftName==DriftID)
    DriftTimes$id<-paste0(DriftID,'_',seq(1:nrow(DriftTimes)))
      if(substr(DriftID,1,4)=='CCES'){
          DriftTimes$end<-DriftTimes$start + seconds(119) }
    ```

3.  **Clean click detections**

```{r, Clean click detections}
#Keep click detections from one channel (upper hydrophone = HTI-92-WB)
data<-filter(data, Channel==1)

#Omit click detectors 0 and 1
data<-filter(data,detectorName!='Click_Detector_0')
data<-filter(data,detectorName!='Click_Detector_1')
data<-filter(data, peak>15 & duration<2000)
ClickData<-getClickData(data)

#Keep subset of data for events with more than 100,000 clicks
TooBig<-ClickData %>%
  group_by(eventId) %>%
  summarize(TotalClicks = n()) %>%
  filter(TotalClicks>100000) 

#For events with large sample sizes, randomly select 80% of clicks to remove
for(b in 1:nrow(TooBig)){
set.seed(1)

IgnoreClicks<-ClickData %>%
  filter(eventId==TooBig$eventId[b]) %>%
  slice_sample(prop=0.80)

data<-data %>%
  filter(!UID %in% IgnoreClicks$UID)
ClickData<-ClickData %>% 
  filter(!UID %in% IgnoreClicks$UID)
}
```

4.  **Add known events to a new copy of the database**

```{r}

CD <- as.data.table(ClickData)
setkey(CD, eventId)
uid_by_event <- CD[, .(UIDs = list(UID)), by = eventId]
uid_lookup <- setNames(uid_by_event$UIDs, uid_by_event$eventId)

db_path <- "C:/Users/annes/Documents/Github/Rissos_Geographic_Variation/data/Databases/ADRIFT_055 - Copy.sqlite3"   # <- must be a single character string


for (i in seq_len(nrow(DriftTimes))) {
  id <- DriftTimes$id[i]
  addPgEvent(
    db_path,  # or your original PAMpal db handle variable
    binary    = bin_by_event[[as.character(id)]],
    UIDs      = uid_lookup[[as.character(id)]],
    eventType = "Gg Click Event",
    start     = DriftTimes$start[i],
    end       = DriftTimes$end[i],
    type      = "click"
  )
}

```

5.  **Review the event mean spectra**

```{r, Plot mean spectra}

#Plot mean spectrum & concatenated spectrogram
calculateAverageSpectra(data,flim=c(0,100000))
beep()
```
